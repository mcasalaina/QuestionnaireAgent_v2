{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848a073",
   "metadata": {},
   "source": [
    "# Questionnaire Agent Evaluation\n",
    "\n",
    "This notebook evaluates the Questionnaire Agent using a comprehensive set of test queries focused on Azure AI topics.\n",
    "The evaluation includes both Azure AI-specific questions and general questions to test the agent's ability to stay on topic and provide accurate, contextually relevant responses.\n",
    "\n",
    "This evaluation system uses Azure AI evaluation SDK to assess:\n",
    "- **Relevance**: How relevant responses are to the queries\n",
    "- **Coherence**: How logically structured and consistent responses are\n",
    "- **Fluency**: How well-written and readable responses are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03554da",
   "metadata": {},
   "source": [
    "## Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e235d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìÅ Working directory: c:\\src\\QuestionnaireAgent_v2\\evaluation\n",
      "üìÇ Parent directory added to path: c:\\src\\QuestionnaireAgent_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure AI evaluation imports\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "\n",
    "# Azure authentication and client imports\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Add the parent directory to sys.path to import the questionnaire agent\n",
    "parent_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÇ Parent directory added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b17ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded:\n",
      "  Azure OpenAI Endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n",
      "  Main Model Deployment: gpt-4.1\n",
      "  Evaluation Model: gpt-4o-mini\n",
      "  Evaluation Endpoint: https://aipmaker-project-resource.openai.azure.com/\n",
      "  API Version: 2024-05-01-preview\n",
      "  Bing Connection ID: aipmakerbingsearch\n",
      "‚úÖ All required environment variables are configured!\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "\n",
    "# Evaluation model configuration (use the same or different model for evaluation)\n",
    "EVALUATION_MODEL = os.getenv(\"EVALUATION_MODEL\", AZURE_OPENAI_MODEL_DEPLOYMENT)\n",
    "EVALUATION_MODEL_ENDPOINT = os.getenv(\"EVALUATION_MODEL_ENDPOINT\", AZURE_OPENAI_ENDPOINT)\n",
    "EVALUATION_OPENAI_API_VERSION = os.getenv(\"EVALUATION_OPENAI_API_VERSION\", AZURE_OPENAI_API_VERSION)\n",
    "\n",
    "# Bing Search configuration\n",
    "BING_CONNECTION_ID = os.getenv(\"BING_CONNECTION_ID\")\n",
    "\n",
    "# Application Insights for tracing\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING = os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"  Main Model Deployment: {AZURE_OPENAI_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Model: {EVALUATION_MODEL}\")\n",
    "print(f\"  Evaluation Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "print(f\"  API Version: {AZURE_OPENAI_API_VERSION}\")\n",
    "print(f\"  Bing Connection ID: {BING_CONNECTION_ID}\")\n",
    "\n",
    "# Verify required configurations\n",
    "missing_configs = []\n",
    "if not AZURE_OPENAI_ENDPOINT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_ENDPOINT\")\n",
    "if not AZURE_OPENAI_MODEL_DEPLOYMENT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n",
    "if not BING_CONNECTION_ID:\n",
    "    missing_configs.append(\"BING_CONNECTION_ID\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing_configs)}\")\n",
    "    print(\"   Please check your .env file configuration.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fb67f",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44bd41a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Project Client initialized successfully!\n",
      "üîó Connected to endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure AI Project Client for evaluation\n",
    "try:\n",
    "    # Use Azure CLI credentials for authentication\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    # Initialize the Azure AI Project Client\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Azure AI Project Client initialized successfully!\")\n",
    "    print(f\"üîó Connected to endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure AI Project Client: {str(e)}\")\n",
    "    print(\"   Please ensure you are logged in with 'az login' and have proper permissions.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e56b",
   "metadata": {},
   "source": [
    "## Create Questionnaire Agent Query Function\n",
    "\n",
    "This function serves as the target for the evaluation system. It will use the existing questionnaire agent to process queries and return responses in the format expected by the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365818fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported QuestionnaireAgentUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:question_answerer:‚úÖ Azure AI Foundry tracing initialized successfully (content recording: enabled).\n",
      "INFO:question_answerer:Connecting to Azure AI Foundry endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n",
      "INFO:question_answerer:Azure AI Project Client initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Questionnaire agent initialized in headless mode\n"
     ]
    }
   ],
   "source": [
    "# Import the questionnaire agent\n",
    "try:\n",
    "    from question_answerer import QuestionnaireAgentUI\n",
    "    print(\"‚úÖ Successfully imported QuestionnaireAgentUI\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import QuestionnaireAgentUI: {str(e)}\")\n",
    "    print(\"   Make sure the question_answerer.py file is in the parent directory\")\n",
    "    raise\n",
    "\n",
    "# Initialize the questionnaire agent in headless mode for evaluation\n",
    "try:\n",
    "    questionnaire_agent = QuestionnaireAgentUI(\n",
    "        headless_mode=True, \n",
    "        max_retries=3,  # Limit retries for faster evaluation\n",
    "        mock_mode=False  # Use real Azure AI services for evaluation\n",
    "    )\n",
    "    print(\"‚úÖ Questionnaire agent initialized in headless mode\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize questionnaire agent: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:question_answerer:Agent cleanup completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the query function with a sample question...\n",
      "‚úÖ Test query: What are the key features of Azure AI?\n",
      "üìù Response preview: Azure AI provides a suite of cloud-based artificial intelligence services designed to help developers and organizations build, deploy, and manage AI-powered applications efficiently. The key features ...\n",
      "üìã Context: Question processed in context: Microsoft Azure AI. Links found: https://azure.microsoft.com/en-us/solutions/ai/\n"
     ]
    }
   ],
   "source": [
    "def query_questionnaire_agent(query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Target function for evaluation that queries the questionnaire agent.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask the agent\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary containing query and response for evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use default context and parameters for evaluation\n",
    "        context = \"Microsoft Azure AI\"\n",
    "        char_limit = 2000\n",
    "        max_retries = 3\n",
    "        verbose = False\n",
    "        \n",
    "        # Process the query using the questionnaire agent\n",
    "        success, answer, links = questionnaire_agent.process_single_question_cli(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            char_limit=char_limit,\n",
    "            verbose=verbose,\n",
    "            max_retries=max_retries\n",
    "        )\n",
    "        \n",
    "        if success and answer:\n",
    "            # Return in the format expected by the evaluation framework\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": answer\n",
    "            }\n",
    "        else:\n",
    "            # Handle case where agent failed to generate a response\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"The agent was unable to generate a response for this query.\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying agent for '{query[:50]}...': {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error occurred while processing query: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Test the function with a sample query\n",
    "print(\"üß™ Testing the query function with a sample question...\")\n",
    "test_result = query_questionnaire_agent(\"What are the key features of Azure AI?\")\n",
    "print(f\"‚úÖ Test query: {test_result['query']}\")\n",
    "print(f\"üìù Response preview: {test_result['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55356cca",
   "metadata": {},
   "source": [
    "## Configure AI Evaluation Models\n",
    "\n",
    "Set up the Azure OpenAI model configuration and initialize the evaluation metrics that will assess the quality of the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a78bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model configuration created for evaluation\n",
      "   Endpoint: https://aipmaker-project-resource.openai.azure.com/\n",
      "   Deployment: gpt-4o-mini\n",
      "   API Version: 2024-12-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure OpenAI model for evaluation\n",
    "# Note: We need to use API key authentication for the evaluation models\n",
    "# since the evaluation SDK requires explicit API keys\n",
    "\n",
    "# Try to get API key from environment\n",
    "EVALUATION_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "if not EVALUATION_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Warning: No Azure OpenAI API key found in environment variables.\")\n",
    "    print(\"   The evaluation will attempt to use Azure CLI credentials, but may need API key.\")\n",
    "    print(\"   Consider setting AZURE_OPENAI_KEY in your .env file.\")\n",
    "\n",
    "# Configure the evaluation model\n",
    "try:\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=EVALUATION_MODEL_ENDPOINT,\n",
    "        azure_deployment=EVALUATION_MODEL,\n",
    "        api_version=EVALUATION_OPENAI_API_VERSION,\n",
    "        api_key=EVALUATION_API_KEY\n",
    "    )\n",
    "    print(f\"‚úÖ Model configuration created for evaluation\")\n",
    "    print(f\"   Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "    print(f\"   Deployment: {EVALUATION_MODEL}\")\n",
    "    print(f\"   API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create model configuration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ca7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing evaluation metrics...\n",
      "‚úÖ Relevance evaluator initialized\n",
      "‚úÖ Coherence evaluator initialized\n",
      "‚úÖ Fluency evaluator initialized\n",
      "\n",
      "üéØ All evaluators configured successfully!\n",
      "   Each evaluator will assess different aspects of response quality:\n",
      "   ‚Ä¢ Groundedness: How well responses are based on context\n",
      "   ‚Ä¢ Relevance: How relevant responses are to queries\n",
      "   ‚Ä¢ Coherence: How logically structured responses are\n",
      "   ‚Ä¢ Fluency: How well-written and readable responses are\n"
     ]
    }
   ],
   "source": [
    "# Initialize all evaluators\n",
    "try:\n",
    "    print(\"üîß Initializing evaluation metrics...\")\n",
    "    \n",
    "    # Relevance: Measures how relevant the response is to the query\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Relevance evaluator initialized\")\n",
    "    \n",
    "    # Coherence: Measures the logical flow and consistency of the response\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Coherence evaluator initialized\")\n",
    "    \n",
    "    # Fluency: Measures the readability and linguistic quality of the response\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Fluency evaluator initialized\")\n",
    "    \n",
    "    print(\"\\nüéØ All evaluators configured successfully!\")\n",
    "    print(\"   Each evaluator will assess different aspects of response quality:\")\n",
    "    print(\"   ‚Ä¢ Relevance: How relevant responses are to queries\")\n",
    "    print(\"   ‚Ä¢ Coherence: How logically structured responses are\")\n",
    "    print(\"   ‚Ä¢ Fluency: How well-written and readable responses are\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring evaluators: {str(e)}\")\n",
    "    print(\"   This might be due to API key issues or model configuration problems.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65040bf1",
   "metadata": {},
   "source": [
    "## Load Test Queries from JSONL File\n",
    "\n",
    "Load the evaluation queries that include both Azure AI-specific questions and general questions to comprehensively test the agent's capabilities and topic adherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91fe423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 129 evaluation queries\n",
      "üìä Query breakdown:\n",
      "   ‚Ä¢ Azure AI related queries: 57\n",
      "   ‚Ä¢ General/off-topic queries: 72\n",
      "   ‚Ä¢ Total queries: 129\n",
      "\n",
      "üìù Sample Azure AI queries:\n",
      "   1. What are the key features of Azure AI?\n",
      "   2. How does Azure OpenAI work?\n",
      "   3. What is the difference between Azure AI and OpenAI?\n",
      "\n",
      "üìù Sample general queries:\n",
      "   1. What is the weather like in Paris today?\n",
      "   2. How do I cook pasta?\n",
      "   3. What's the capital of France?\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation queries from JSONL file\n",
    "evaluation_queries_file = \"evaluation_queries.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not Path(evaluation_queries_file).exists():\n",
    "        print(f\"‚ùå Evaluation queries file not found: {evaluation_queries_file}\")\n",
    "        print(f\"   Current working directory: {Path.cwd()}\")\n",
    "        print(f\"   Looking for file at: {Path(evaluation_queries_file).absolute()}\")\n",
    "        raise FileNotFoundError(f\"Could not find {evaluation_queries_file}\")\n",
    "    \n",
    "    # Read the JSONL file\n",
    "    queries = []\n",
    "    with open(evaluation_queries_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    query_data = json.loads(line)\n",
    "                    if 'query' in query_data:\n",
    "                        queries.append(query_data['query'])\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Warning: Line {line_num} missing 'query' field: {line}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  Warning: Invalid JSON on line {line_num}: {line}\")\n",
    "                    print(f\"   Error: {str(e)}\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {len(queries)} evaluation queries\")\n",
    "    \n",
    "    # Analyze the types of queries\n",
    "    azure_ai_queries = [q for q in queries if any(keyword in q.lower() for keyword in ['azure', 'ai', 'openai', 'cognitive', 'machine learning'])]\n",
    "    general_queries = [q for q in queries if q not in azure_ai_queries]\n",
    "    \n",
    "    print(f\"üìä Query breakdown:\")\n",
    "    print(f\"   ‚Ä¢ Azure AI related queries: {len(azure_ai_queries)}\")\n",
    "    print(f\"   ‚Ä¢ General/off-topic queries: {len(general_queries)}\")\n",
    "    print(f\"   ‚Ä¢ Total queries: {len(queries)}\")\n",
    "    \n",
    "    # Show some sample queries\n",
    "    print(f\"\\nüìù Sample Azure AI queries:\")\n",
    "    for i, query in enumerate(azure_ai_queries[:3], 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    print(f\"\\nüìù Sample general queries:\")\n",
    "    for i, query in enumerate(general_queries[:3], 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading evaluation queries: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d1260",
   "metadata": {},
   "source": [
    "## Run Comprehensive Agent Evaluation\n",
    "\n",
    "Execute the evaluation pipeline using all configured evaluators against the test dataset. This process will measure agent performance across multiple dimensions.\n",
    "\n",
    "**Note**: This evaluation may take a significant amount of time depending on the number of queries and the response time of the agent and evaluation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive evaluation: questionnaire_agent_evaluation_20250904_001124\n",
      "üìä Total queries to evaluate: 129\n",
      "‚è∞ Started at: 2025-09-04 00:11:24\n",
      "\n",
      "‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\n",
      "   Each query requires multiple API calls to the agent and evaluation models.\n",
      "‚ùå Evaluation failed: name 'groundedness_evaluator' is not defined\n",
      "\n",
      "üîß Troubleshooting tips:\n",
      "   ‚Ä¢ Ensure Azure CLI authentication is working: 'az login'\n",
      "   ‚Ä¢ Check that all required environment variables are set\n",
      "   ‚Ä¢ Verify Azure OpenAI API quotas and limits\n",
      "   ‚Ä¢ Check network connectivity to Azure services\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'groundedness_evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Configure the evaluators dictionary with proper naming for Azure AI Foundry\u001b[39;00m\n\u001b[32m     13\u001b[39m     evaluators_config = {\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgroundedness\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mgroundedness_evaluator\u001b[49m,\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m: relevance_evaluator,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcoherence\u001b[39m\u001b[33m\"\u001b[39m: coherence_evaluator,\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfluency\u001b[39m\u001b[33m\"\u001b[39m: fluency_evaluator,\n\u001b[32m     18\u001b[39m     }\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Execute the evaluation\u001b[39;00m\n\u001b[32m     21\u001b[39m     evaluation_result = evaluate(\n\u001b[32m     22\u001b[39m         data=evaluation_queries_file,\n\u001b[32m     23\u001b[39m         target=query_questionnaire_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m         evaluation_name=evaluation_name,\n\u001b[32m     27\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'groundedness_evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare for evaluation\n",
    "evaluation_name = f\"questionnaire_agent_evaluation_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üöÄ Starting comprehensive evaluation: {evaluation_name}\")\n",
    "print(f\"üìä Total queries to evaluate: {len(queries)}\")\n",
    "print(f\"‚è∞ Started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\")\n",
    "print(\"   Each query requires multiple API calls to the agent and evaluation models.\")\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    # Configure the evaluators dictionary with proper naming for Azure AI Foundry\n",
    "    evaluators_config = {\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "    }\n",
    "    \n",
    "    # Execute the evaluation\n",
    "    evaluation_result = evaluate(\n",
    "        data=evaluation_queries_file,\n",
    "        target=query_questionnaire_agent,\n",
    "        evaluators=evaluators_config,\n",
    "        azure_ai_project=AZURE_OPENAI_ENDPOINT,  # Azure AI project endpoint\n",
    "        evaluation_name=evaluation_name,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"‚è∞ Finished at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Display Azure AI Foundry Studio URL if available\n",
    "    if 'studio_url' in evaluation_result:\n",
    "        print(f\"üîó Azure AI Foundry Studio URL: {evaluation_result['studio_url']}\")\n",
    "        print(\"   You can view detailed results and visualizations in the Azure AI Foundry portal.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   ‚Ä¢ Ensure Azure CLI authentication is working: 'az login'\")\n",
    "    print(\"   ‚Ä¢ Check that all required environment variables are set\")\n",
    "    print(\"   ‚Ä¢ Verify Azure OpenAI API quotas and limits\")\n",
    "    print(\"   ‚Ä¢ Check network connectivity to Azure services\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54140a",
   "metadata": {},
   "source": [
    "## Process and Display Evaluation Results\n",
    "\n",
    "Extract and format evaluation metrics, display summary statistics, and provide detailed analysis of agent performance across different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display evaluation metrics\n",
    "print(\"üìä EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get the overall metrics\n",
    "    metrics = evaluation_result.get(\"metrics\", {})\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"‚ö†Ô∏è  No metrics found in evaluation results\")\n",
    "        print(\"   This might indicate an issue with the evaluation process\")\n",
    "    else:\n",
    "        print(\"\\nüéØ Overall Performance Metrics:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            # Format the metric value based on its type\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value}\")\n",
    "        \n",
    "        # Calculate and display average score\n",
    "        numeric_metrics = {k: v for k, v in metrics.items() if isinstance(v, (int, float))}\n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            print(f\"\\nüìà Average Score Across All Metrics: {avg_score:.4f}\")\n",
    "            \n",
    "            # Provide interpretation\n",
    "            print(f\"\\nüìù Performance Interpretation:\")\n",
    "            if avg_score >= 4.0:\n",
    "                print(\"   üü¢ Excellent: The agent demonstrates high-quality responses\")\n",
    "            elif avg_score >= 3.0:\n",
    "                print(\"   üü° Good: The agent performs well with room for improvement\")\n",
    "            elif avg_score >= 2.0:\n",
    "                print(\"   üü† Fair: The agent shows moderate performance, needs improvement\")\n",
    "            else:\n",
    "                print(\"   üî¥ Poor: The agent requires significant improvements\")\n",
    "        \n",
    "        # Analyze individual metrics\n",
    "        print(f\"\\nüîç Detailed Metric Analysis:\")\n",
    "        for metric_name, metric_value in numeric_metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                if metric_value >= 4.0:\n",
    "                    status = \"üü¢ Excellent\"\n",
    "                elif metric_value >= 3.0:\n",
    "                    status = \"üü° Good\"\n",
    "                elif metric_value >= 2.0:\n",
    "                    status = \"üü† Fair\"\n",
    "                else:\n",
    "                    status = \"üî¥ Poor\"\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f} - {status}\")\n",
    "    \n",
    "    # Display additional result information\n",
    "    if 'outputs' in evaluation_result:\n",
    "        outputs = evaluation_result['outputs']\n",
    "        print(f\"\\nüìã Detailed Results Available:\")\n",
    "        print(f\"   ‚Ä¢ Number of evaluated samples: {len(outputs) if hasattr(outputs, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    # Display data info if available\n",
    "    if 'data' in evaluation_result:\n",
    "        print(f\"\\nüìÅ Evaluation Data Info:\")\n",
    "        print(f\"   ‚Ä¢ Data source: {evaluation_queries_file}\")\n",
    "        print(f\"   ‚Ä¢ Total queries processed: {len(queries)}\")\n",
    "        print(f\"   ‚Ä¢ Azure AI queries: {len(azure_ai_queries)}\")\n",
    "        print(f\"   ‚Ä¢ General queries: {len(general_queries)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing evaluation results: {str(e)}\")\n",
    "    print(\"   Raw evaluation result keys:\", list(evaluation_result.keys()) if evaluation_result else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a51bf",
   "metadata": {},
   "source": [
    "## Export Results for Analysis\n",
    "\n",
    "Save evaluation results to files, generate reports, and provide information for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41697c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation results to files\n",
    "results_dir = Path(\"evaluation_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "summary_file = results_dir / f\"evaluation_summary_{timestamp}.txt\"\n",
    "\n",
    "try:\n",
    "    # Save detailed results as JSON\n",
    "    print(f\"üíæ Saving detailed results to: {results_file}\")\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"evaluation_name\": evaluation_name,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"configuration\": {\n",
    "                \"model_deployment\": AZURE_OPENAI_MODEL_DEPLOYMENT,\n",
    "                \"evaluation_model\": EVALUATION_MODEL,\n",
    "                \"total_queries\": len(queries),\n",
    "                \"azure_ai_queries\": len(azure_ai_queries),\n",
    "                \"general_queries\": len(general_queries)\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"evaluation_result\": evaluation_result\n",
    "        }, f, indent=2, default=str)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"üìÑ Generating summary report: {summary_file}\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"QUESTIONNAIRE AGENT EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Evaluation Name: {evaluation_name}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Queries Evaluated: {len(queries)}\\n\")\n",
    "        f.write(f\"Azure AI Related Queries: {len(azure_ai_queries)}\\n\")\n",
    "        f.write(f\"General/Off-topic Queries: {len(general_queries)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                f.write(f\"{metric_name.title()}: {metric_value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric_name.title()}: {metric_value}\\n\")\n",
    "        \n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            f.write(f\"\\nAverage Score: {avg_score:.4f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nPerformance Level: \")\n",
    "            if avg_score >= 4.0:\n",
    "                f.write(\"Excellent\\n\")\n",
    "            elif avg_score >= 3.0:\n",
    "                f.write(\"Good\\n\")\n",
    "            elif avg_score >= 2.0:\n",
    "                f.write(\"Fair\\n\")\n",
    "            else:\n",
    "                f.write(\"Poor\\n\")\n",
    "        \n",
    "        if 'studio_url' in evaluation_result:\n",
    "            f.write(f\"\\nAzure AI Foundry Studio URL:\\n{evaluation_result['studio_url']}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Results exported successfully!\")\n",
    "    print(f\"üìÅ Results directory: {results_dir.absolute()}\")\n",
    "    print(f\"   ‚Ä¢ Detailed JSON: {results_file.name}\")\n",
    "    print(f\"   ‚Ä¢ Summary report: {summary_file.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting results: {str(e)}\")\n",
    "    print(\"   Results are still available in the evaluation_result variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41476",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This evaluation notebook provides a comprehensive assessment of the Questionnaire Agent's performance across multiple dimensions:\n",
    "\n",
    "### What This Evaluation Measures:\n",
    "\n",
    "1. **Relevance**: How relevant and on-topic the responses are to the input queries\n",
    "2. **Coherence**: How logically structured and internally consistent the responses are\n",
    "3. **Fluency**: How well-written, readable, and linguistically sound the responses are\n",
    "\n",
    "### Evaluation Dataset:\n",
    "\n",
    "- **Azure AI Queries**: Tests the agent's knowledge and accuracy on its primary domain\n",
    "- **General Queries**: Tests the agent's ability to stay on topic and handle off-domain questions appropriately\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Objective Assessment**: Uses AI-assisted evaluation for consistent, scalable assessment\n",
    "- **Multi-dimensional Analysis**: Evaluates different aspects of response quality\n",
    "- **Azure Integration**: Results are available in Azure AI Foundry Studio for detailed analysis\n",
    "- **Reproducible**: Can be run repeatedly to track improvements over time\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review Detailed Results**: Use the Azure AI Foundry Studio URL to explore individual query results\n",
    "2. **Identify Improvement Areas**: Focus on metrics with lower scores\n",
    "3. **Iterative Development**: Re-run evaluation after making improvements to track progress\n",
    "4. **Expand Evaluation**: Consider adding more specific test cases or custom evaluators\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- Detailed JSON results for programmatic analysis\n",
    "- Human-readable summary reports\n",
    "- Links to Azure AI Foundry Studio for interactive exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
